{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0d41d7a-1082-48f7-8c2c-755dcf2cc641",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b64efc32-178f-4f9e-ab88-cbc0d0c1c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c838cfb5-65e7-4951-847a-ef832967cf05",
   "metadata": {},
   "source": [
    "### Downloading nltk Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dacec805-b8b7-45b6-9539-7f19fe46c54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Swarnadwip_Sarkar\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Swarnadwip_Sarkar\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Swarnadwip_Sarkar\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0ec569-5b39-445b-872d-09c3575f5d98",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40f9ed31-095d-4236-92b9-cf8ffe7fa2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "624efaeb-a514-45b4-862d-94cfde317d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2984fa9-1363-47a5-be0b-a50f7a3a624d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "801311cc-600f-4d4f-b433-5e47238ea304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66bdfe7-8123-494e-88e9-32ee3f543ad8",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d52a312-283c-4462-afa8-9da655498332",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = string.punctuation\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "lm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4c2b68e-83ff-454b-9c8f-82c77f642786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master\\'s of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell\\'s murals decorating every surface) are terribly well done.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = df['review'][1]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e50ad4c-cbae-41c8-a531-06863d883bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A wonderful little production. The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master\\'s of comedy and his life. The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell\\'s murals decorating every surface) are terribly well done.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BeautifulSoup(text,\"html.parser\").get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "624e7f76-bf45-46dc-92a7-37ebd5610ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Removing all html tags\n",
    "    only_text = BeautifulSoup(text,\"html.parser\").get_text()\n",
    "    # Lowercasing and removing punctuation\n",
    "    text = \"\".join([char.lower() for char in only_text if char not in punctuation])\n",
    "    tokens = word_tokenize(text)\n",
    "    # First removing stopwords and then applying lemmatization\n",
    "    final_text = \" \".join([lm.lemmatize(word) for word in tokens if word not in stopwords])\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71ebf951-cfcd-4188-a14e-026347ef1fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wonderful little production filming technique unassuming oldtimebbc fashion give comforting sometimes discomforting sense realism entire piece actor extremely well chosen michael sheen got polari voice pat truly see seamless editing guided reference williams diary entry well worth watching terrificly written performed piece masterful production one great master comedy life realism really come home little thing fantasy guard rather use traditional dream technique remains solid disappears play knowledge sens particularly scene concerning orton halliwell set particularly flat halliwells mural decorating every surface terribly well done'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(df['review'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96a994ad-f60f-4098-a6c6-297d1b7708b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one reviewer mentioned watching 1 oz episode y...\n",
       "1        wonderful little production filming technique ...\n",
       "2        thought wonderful way spend time hot summer we...\n",
       "3        basically there family little boy jake think t...\n",
       "4        petter matteis love time money visually stunni...\n",
       "                               ...                        \n",
       "49995    thought movie right good job wasnt creative or...\n",
       "49996    bad plot bad dialogue bad acting idiotic direc...\n",
       "49997    catholic taught parochial elementary school nu...\n",
       "49998    im going disagree previous comment side maltin...\n",
       "49999    one expects star trek movie high art fan expec...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df['review'].apply(lambda x: clean_text(x))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d130eaf9-81d8-4605-b252-c15e65505284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        1\n",
       "2        1\n",
       "3        0\n",
       "4        1\n",
       "        ..\n",
       "49995    1\n",
       "49996    0\n",
       "49997    0\n",
       "49998    0\n",
       "49999    0\n",
       "Name: sentiment, Length: 50000, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = df['sentiment'].apply(lambda x: 1 if x=='positive' else 0)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82372687-64ab-40f5-a603-23eadd65d27e",
   "metadata": {},
   "source": [
    "### Split Data in Training and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ffc9877-bd0b-4fe8-814b-02c10ea42b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1212a680-6403-4857-ab37-048fe3f60829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37500,), (37500,))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2061058-6de7-4108-b6ed-19767a18a57b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12500,), (12500,))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dd2994ec-1f4c-4288-a26e-c2a4fc9825ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20975    say truth went see movie nicolas cage favorite...\n",
       "33031    mindless movie piece crap boring like full hou...\n",
       "4191     obviously film great influence buddy genre act...\n",
       "27069    saddest thing tribute almost singer including ...\n",
       "20238    overrated shortlived series measly two season ...\n",
       "                               ...                        \n",
       "29083    lose friend alienate people came 2008 bombed u...\n",
       "10       phil alien one quirky film humour based around...\n",
       "19459    occasionally talk show good topical debate spa...\n",
       "5798     wow even discus movie without tear coming eye ...\n",
       "37397    kim basinger mickey rourke star controversial ...\n",
       "Name: review, Length: 37500, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d57610fe-865f-431e-8c13-f48565d85934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20975    1\n",
       "33031    0\n",
       "4191     1\n",
       "27069    0\n",
       "20238    0\n",
       "        ..\n",
       "29083    1\n",
       "10       0\n",
       "19459    0\n",
       "5798     1\n",
       "37397    0\n",
       "Name: sentiment, Length: 37500, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "68a058ec-d15d-457b-8a37-b6e93fec55d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119.84288"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average Number of Tokens\n",
    "sum = 0\n",
    "for i in X.to_list():\n",
    "    sum += len(i.split(\" \"))\n",
    "avg = sum / len(X.to_list())\n",
    "avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d6a746-0205-4a83-9fd5-2c56545241ca",
   "metadata": {},
   "source": [
    "### Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4620b0f2-bc58-4be2-9101-0657e6c1626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c18d68b3-48dd-4249-8ee6-ecad18c83c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "max_len = 200\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens = vocab_size,\n",
    "    output_mode = 'int',\n",
    "    output_sequence_length = max_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "68572b95-4814-4b82-9c18-64148681e4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Vocabulary\n",
    "vectorize_layer.adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7a489cb5-2b0a-4393-bc08-e9953be4f351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', '[UNK]', 'movie', 'film', 'one', 'like', 'time', 'good',\n",
       "       'character', 'get', 'even', 'story', 'would', 'see', 'make',\n",
       "       'really', 'scene', 'much', 'well', 'people', 'great', 'bad',\n",
       "       'also', 'show', 'first', 'dont', 'way', 'thing', 'made', 'could',\n",
       "       'think', 'life', 'go', 'know', 'watch', 'love', 'many', 'seen',\n",
       "       'actor', 'two', 'plot', 'never', 'say', 'look', 'end', 'little',\n",
       "       'acting', 'best', 'year', 'ever'], dtype='<U18')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = np.array(vectorize_layer.get_vocabulary())\n",
    "vocab[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6fa1ab90-35e4-4f0b-9d21-30ea51c9916d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5efebbb0-1666-4bbb-9ffb-af1f89d1dc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_training_set = vectorize_layer(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d2190cc4-ed84-4705-a10f-5c3ef3154ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37500,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "73c88aef-6a1c-44de-b900-b537e41a075d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([37500, 200])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_training_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b3f88397-2bef-4584-af95-396dd13bf41a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(37500, 200), dtype=int64, numpy=\n",
       "array([[  42,  718,  313, ...,    0,    0,    0],\n",
       "       [2553,    2,  233, ...,    0,    0,    0],\n",
       "       [ 429,    3,   20, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [1665,  443,   23, ...,    6,   51,  853],\n",
       "       [1352,   10, 3738, ...,    0,    0,    0],\n",
       "       [2625, 6981, 2247, ...,    0,    0,    0]])>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2476692a-25d1-46ea-b69b-c5cc8b4a3210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', '[UNK]', 'movie', ..., 'mccormick', 'mccann', 'mcbride'],\n",
       "      dtype='<U18')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bbfe6deb-a119-4180-9231-aa23065935f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'obviously film great influence buddy genre action genre well george lucas fan flick much star war series seems homage gunga din character grant mclaglen fairbanks play precursor han solo luke skywalker chewbacca even sam jaffes gunga din morphed c3po r2d2 like jar jar binkstoday film viewed non pc speech eduardo ciannelli guru leader indian opposition british raj could echoed sentiment many today young boy great film three strong male lead hint romance time young boy deemed kissing girl saturday matinee film mush like today skin greeted delight late lament lost innocencehopefully film forgotten channel surfing stop tcm catch film action adventure cast thousand instead cgi actor'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.to_list()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "93795bf2-2644-4e12-80d0-0b4215f12857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(200,), dtype=int64, numpy=\n",
       "array([  429,     3,    20,  1850,  1239,   405,   106,   405,    18,\n",
       "         591,  3298,   109,   300,    17,   111,   188,   120,    97,\n",
       "        2528,  9801,  7650,     8,  2161,  6733,  8286,    71, 10406,\n",
       "        4550,  3929,  2502, 11986,     1,    10,  1085,     1,  9801,\n",
       "        7650, 15784,     1,     1,     5,  7354,  7354,     1,     3,\n",
       "        2107,  2831,  5734,  1604, 18513,     1,  8910,  1401,   913,\n",
       "        7905,   588,  7174,    29, 16489,  4227,    36,   397,    98,\n",
       "         204,    20,     3,   197,   467,   708,   216,  1940,   737,\n",
       "           6,    98,   204,  7121,  4836,    84,  2160,     1,     3,\n",
       "       15273,     5,   397,  2174, 13393,  2474,   441, 11772,   322,\n",
       "           1,     3,  1412,   879,  4913,   427,  5531,   960,     3,\n",
       "         106,   821,    89,  1583,   206,  1399,    38,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0])>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_training_set[2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2f98256d-f5e2-4f84-b72a-144da1a78592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3]),)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(vocab=='film')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "622e4086-4196-4f52-9322-c50ad1153078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b8632d-064f-433c-ad37-057ef3b110c1",
   "metadata": {},
   "source": [
    "### Building Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "233019c8-c747-4f99-8993-20361f6308e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "embed = Embedding(\n",
    "    input_dim = len(vocab),\n",
    "    output_dim = 100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "25b1f6ae-c162-4526-a6f2-05d63900fb5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(200, 100), dtype=float32, numpy=\n",
       "array([[-0.00443739, -0.03519762,  0.04777107, ...,  0.0401245 ,\n",
       "         0.0066235 ,  0.0440085 ],\n",
       "       [-0.04792074,  0.00306188, -0.01716427, ...,  0.04804263,\n",
       "        -0.04642345,  0.00703468],\n",
       "       [-0.04668491, -0.00609167, -0.03071828, ..., -0.03132644,\n",
       "         0.02055762,  0.03676274],\n",
       "       ...,\n",
       "       [-0.01740215, -0.02308092,  0.02307146, ...,  0.03784061,\n",
       "        -0.04945008, -0.00153155],\n",
       "       [-0.01740215, -0.02308092,  0.02307146, ...,  0.03784061,\n",
       "        -0.04945008, -0.00153155],\n",
       "       [-0.01740215, -0.02308092,  0.02307146, ...,  0.03784061,\n",
       "        -0.04945008, -0.00153155]], dtype=float32)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed(vectorize_training_set[2,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29ed3a8-7bfd-4d8c-b070-052fb693bf72",
   "metadata": {},
   "source": [
    "### Building LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "53400014-d568-4879-a88b-51659d6beceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TextVectorization name=text_vectorization, built=True>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3500673c-3a1d-498a-b8e3-1a2e75dbdb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import LSTM,Dense,Dropout\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(1,), dtype = tf.string))\n",
    "\n",
    "model.add(vectorize_layer)\n",
    "model.add(Embedding(input_dim = vocab_size, output_dim = 100))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d5fd6450-5bb8-465f-bc83-a181632a0262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">80,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,464</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTextVectorization\u001b[0m)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │     \u001b[38;5;34m2,000,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m80,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m6,464\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,086,929</span> (7.96 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,086,929\u001b[0m (7.96 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,086,929</span> (7.96 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,086,929\u001b[0m (7.96 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "476442b8-4f99-4a9d-9032-7fb2a5c50e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d4dea7-d458-4253-a84c-ac0c501c9681",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fbffb64f-2194-4714-8690-73757c1041ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling TextVectorization.call().\n\n\u001b[1mWhen using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(None, 200) with rank=2\u001b[0m\n\nArguments received by TextVectorization.call():\n  • inputs=tf.Tensor(shape=(None, 200), dtype=string)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m X_train_vec \u001b[38;5;241m=\u001b[39m vectorize_layer(X_train)\n\u001b[0;32m     10\u001b[0m X_test_vec \u001b[38;5;241m=\u001b[39m vectorize_layer(X_test)\n\u001b[1;32m---> 11\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\src\\layers\\preprocessing\\text_vectorization.py:542\u001b[0m, in \u001b[0;36mTextVectorization._preprocess\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 542\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    543\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen using `TextVectorization` to tokenize strings, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    544\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe input rank must be 1 or the last shape dimension \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    545\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust be 1. Received: inputs.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    546\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith rank=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    547\u001b[0m         )\n\u001b[0;32m    548\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    549\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msqueeze(inputs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling TextVectorization.call().\n\n\u001b[1mWhen using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(None, 200) with rank=2\u001b[0m\n\nArguments received by TextVectorization.call():\n  • inputs=tf.Tensor(shape=(None, 200), dtype=string)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=2,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "X_train_vec = vectorize_layer(X_train)\n",
    "X_test_vec = vectorize_layer(X_test)\n",
    "history = model.fit(X_train_vec,Y_train,validation_data=(X_test_vec,Y_test),batch_size=32,epochs=10,\n",
    "                    callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3664dd45-4b86-461d-870d-45a4aeac67f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
